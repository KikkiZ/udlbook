# Chapter 2 Supervised learning

**有监督学习模型**（supervised learning model）定义了从一个或多个输入到一个或多个输出的映射。例如，输入可以是一辆二手丰田普锐斯的车龄和里程，而输出则可以是该车的估值。

该模型只是一个数学方程。当输入通过这个方程时，它会计算输出，而这被称为**推理**（inference）。模型方程还包含了一些**参数**（parameters），不同的参数值会改变计算结果。模型方程描述了输入和输出之间可能的关系族，而参数则会指明具体的关系。

当我们**训练**或**学习**一个模型时，我们会找到描述输入和输出之间真实关系的参数。学习算法会采用由输入-输出对组成的训练集，并调整参数，使得输入能够尽可能准确地预测对应的输出。如果模型在训练集中运行得良好，那么我们希望它在新的输入中能够对未知的输出做出良好的预测。

本章的目标是进一步拓展这些观点。首先，我们会更正式地描述这个框架，并引入一些符号表示。然后，我们将通过一个简单的例子来说明如何使用直线来描述输入和输出之间的关系。这种简单的线性模型既熟悉又易于可视化，但它仍然能够阐明有监督学习的所有主要观点。

## 2.1 Supervised learning overview

在有监督学习中，我们的目标是构建一个模型，该模型接受输入 $x$ 并输出一个预测值 $y$。为了简单起见，我们假设输入 $x$ 和输出 $y$ 都是预先确定且固定大小的向量，并且每个向量的元素总是以相同的顺序排列。在前面二手车的例子中，输入 $x$ 总是包含车辆、里程，并按这个顺序排列。这被称为**结构化**（structured）或**表格化**（tabular）数据。

为了进行预测，我们需要一个模型 $f[\bullet]$ ，它接收输入 $x$ 并返回 $y$，因此有：
$$
y = f[x] \tag{2.1}
$$
当我们根据输入 $x$ 计算预测值 $y$ 时，我们称这个过程为**推断**（inference）。

该模型只是一个具有固定形式的数学方程，它代表输入和输出之间不同关系的一组可能性。模型还包含**参数φ**（parameters）。参数的选择决定了输入和输出之间的具体关系，因此实际上我们应该写成：
$$
y = f[x, \phi]\tag{2.2}
$$
当我们讨论**学习**或**训练**模型时，我们的意思是试图找到参数φ来使输入做出合理的预测输出。我们使用 $I$ 对输入-输出对 $\{x_i, y_i\}$ 组成的**训练集**（training dataset）来学习这些参数。我们的目标是选择能将每个训练输入尽可能准确的映射到对应的输出的参数。我们使用**损失值L**（Loss）来量化映射过程中的不匹配程度。这是一个标量值，它总结了模型在参数φ下从相应输入预测训练输出的不匹配程度。

我们可以将损失视为这些参数的函数 $L[\phi]$。当我们训练模型时，我们实际上是在寻找能使**损失函数**（loss function）最小化的参数，由此可以得到：
$$
\hat{\phi} = argmin_\phi\Big[L[\phi]\Big]\tag{2.3}
$$
如果最小化损失后，损失值很小，那么我们就找到了能够根据训练输入 $x_i$ 准确的预测训练输出 $y_i$ 的模型参数。

在训练完模型之后，我们就需要评估其性能。我们在独立的**测试数据集**（test dataset）上运行模型，以检测在训练期间没有观察到的样例的**泛化**（generalizes）能力。如果性能足够，那我们就可以部署这个模型用于生产环境。

## 2.2 Linear regression example

现在让我们用一个简单的例子来具体解释上面的这些过程。我们将使用一个简单的模型 $y = f[x, \phi]$，该模型根据单个输入 $x$ 预测出单个输出 $y$。然后我们将构建一个损失函数，最后，我们将讨论模型的训练。

### 2.2.1 1D linear regression model

**一维线性回归模型**（1D linear regression model）将输入 $x$ 与输出 $y$​ 之间的关系描述为一条直线，表达式如下：
$$
\begin{align}
y &= f[x, \phi] \\
&= \phi_0 + \phi_1x\tag{2.4}
\end{align}
$$
该模型有两个参数 $\phi = [\phi_0, \phi_1]^T$，其中 $\phi_0$ 为直线的截距（与y轴的交点），$\phi_1$​ 是直线的截距。不同的截距和斜率会导致输入和输出关系的不同，如图2.1所示。因此，公式2.4定义了一组可能的输入-输出关系，而参数的选择决定了这组关系中的某个特定直线。

<img src="https://raw.githubusercontent.com/KikkiZ/ChartBed/main/typora/202401271745124.jpg" alt="1" style="zoom: 50%;" />

[图2.1 线性回归方程 对于给定的参数 $\phi = [\phi_0, \phi_1]^T$，模型根据输入 $x$ 对输出 $y$ 进行预测。不同的截距 $\phi_0$ 和斜率 $\phi_1$ 会改变这些预测结果，图中展示了3组不同的参数对应的预测直线。公式2.4定义了一组可能的输入-输出关系，而参数的选择决定了这组关系中的某个特定直线。]

### 2.2.2 Loss

对于该模型，训练数据集由I个输入-输出对 $\{x_i, y_i\}$​ 组成，如图2.2a所示。图2.2b-d显示了由三组参数定义的三条直线，而在图2.2d中的绿线比其他两条更准确的描述了数据，因为它更接近数据点。然而，我们需要一个确切的判断标准来确定哪些参数φ优于其他参数。为此，我们为每个参数选择分配一个数值，以量化模型和数据之间的不匹配程度。我们将这个值称为**损失值**（loss），较低的损失值通常意味着更好的拟合。

<img src="https://raw.githubusercontent.com/KikkiZ/ChartBed/main/typora/202401272015996.jpg" alt="1" style="zoom:67%;" />

[图2.2 线性回归训练数据、模型和损失 a）训练数据（橙色点）由12对输入-输出对组成。 b-d）每个图显示了具有不同参数的线性回归模型。根据截距和斜率参数的选择，模型的误差可能更大或更小，损失值L是这些误差的平方和。由于模型拟合不良，图b和c中的参数对应的损失值较大，而图d中的损失较小。事实上，在这些所有可能的直线中，图d的参数是的损失值最小，因此是这些是最佳参数。]

这种不匹配时通过模型预测值 $f[x_i, \phi]$ 与真实输出值 $y_i$​ 之间的偏差来捕捉的。这些偏差在图2.2b-d中用橙色的虚线表示。我们将总的损失值、**训练误差**（training error）或损失量化为所有I个训练对的偏差的平方和，由如下公式计算：[question]
$$
\begin{align}
L[\phi] &= \sum^I_{i = 1}\big(f[x_i, \phi] - y_i\big)^2\\
&= \sum^I_{i = 1} \big(\phi_0 + \phi_1 x_i - y_i\big)^2\tag{2.5} 
\end{align}
$$
由于最佳的参数使得这个表达式最小化，我们将其称为**最小二乘损失**（least-squares loss）。平方操作意味着偏差的符号是不重要的。这种选择还有一些理论上的原因，我们将在第5章中继续讨论问题。

损失L是关于参数φ的函数，当模型拟合较差时该值会比较大，如图2.2b和c所示；而当模型拟合较好时该值会比较小，如图2.2d所示。从这个角度考虑，我们将 $L[\phi]$ 称为**损失函数**（loss function）或**成本函数**（cost function），而我们的目标是找到最小化损失值的参数 $\hat{\phi}$ :
$$
\begin{align}
\hat{\phi} &= argmin_\phi\Big[L[\phi]\Big]\\
&= argmin_\phi\Bigg[\sum^I_{i = 1}\Big(f[x_i, \phi] - y_i\Big)^2\Bigg]\\
&= argmin_\phi\Bigg[\sum^I_{i = 1}\Big(\phi_0 + \phi_1x_i - y_i\Big)^2\Bigg]\tag{2.6}
\end{align}
$$
在这个例子中只有两个参数，因此我们可以计算每个组合的损失值，并将损失函数可视化为曲面问题，如图2.3所示。这个”最佳“参数是该曲面的最小值。[Problens 2.1-2.2](#Problems)

![1](https://raw.githubusercontent.com/KikkiZ/ChartBed/main/typora/202401272118400.jpg)

[图2.3 在图2.2a数据集下的线性回归模型的损失函数 a）每个参数组合 $\phi = [\phi_0, \phi_1]$ 都有一个相关的损失。由此产生的损失函数 $L[\phi]$ 可以被可视化为一个曲面。三个标点分别代表图2.2b-d中的三条线。 b）损失值也可以被可视化为一个热力图，其中比较亮的区域代表较大的损失。这个例子里，我们从俯视a图中的曲面，灰色椭圆代表等值线，最佳拟合的直线具有最小的参数，由深绿色圆点表示。]

### 2.2.3 Training

找到最小化损失值的参数的过程称为**模型拟合**（model fitting）、**训练**（training）或是**学习**（learning）。基本方法是随机选择初始参数，然后通过“走下坡路”的方式改进模型的表现，直到我们达到最低点，如图2.4所示。训练的一种方法是测量当前位置曲面的梯度，并朝最陡峭的下坡方向前进一步。然后我们将重复这个过程，直到梯度平缓不能再进一步提高模型的表现。

notes： 这种迭代方式遂于线性回归模型来说并不是必须的，对于这个例子有更简单的方式求得最佳的参数。这种梯度下降的方式适用于更复杂的模型，这类模型由于参数太多，无法用简单的参数排列组合的方式计算损失。

![1](https://raw.githubusercontent.com/KikkiZ/ChartBed/main/typora/202401281539089.jpg)

[图2.4 训练线性回归模型 目标是找到最小损失值对应的两个参数。 a）迭代训练算法随机初始化参数，然后通过“走下坡路”来改进模型，直到它们无法进一步改善为止。在这里，我们从位置0开始，垂直于轮廓向下移动一段距离到位置1。然后我们重新计算下坡方式并移动到位置2，反复多次最终将到函数的最小值。 b）图a中的每个位置0-4对应不同的截距和斜率，因此代表了不同的直线。随着损失的减小，线条与数据的拟合更加紧密。]

### 2.2.4 Testing

在训练完成后，我们想要知道它在真实世界中的表现如何。为了做到这一点，我们需要计算在一个独立的**测试数据集**（test data）上的损失。预测准确性在多大程度上**泛化**（generalizes）到测试数据取决于训练数据的代表性和完整性。然而，这也取决于模型的表达能力。像线性模型这样简单的模型可能无法捕捉输入和输出之间的真实关系，这被称为**欠拟合**（underfitting）。相反，一个非常具有表达力的模型可能会描述非典型的训练数据的统计特性，并导致异常的预测，这被称为**过拟合**（overfitting）

## 2.3 Summary

有监督学习模型是一个函数 $y = f[x, \phi]$，它将输入 $x$ 映射到输出 $y$，具体的关系由参数 $\phi$ 决定。为了训练模型，我们在训练数据集 $\{x_i, y_i\}$ 上义了一个损失函数 $L[\phi]$，用于衡量模型预测值 $f[x_i, \phi]$ 与观察到的输出 $y_i$ 之间的差异，该损失函数是参数 $\phi$​ 的函数。然后，我们寻找损失最小化的参数。我们使用不同的测试数据集来评估模型新输入的泛化能力如何。

第3-9章对上述的这些过程进行了拓展。首先，我们要解决模型本身的问题。线性回归有一个明显的缺点：它只能将输入和输出之间的关系描述为一条直线。**浅层神经网络**（shallow neural networks，Chapter 3）仅比线性回归稍微复杂一些，但可以描述更大范围的输入-输出关系。**深层神经网络**（deep neural networks，Chapter 4）具有同样的表达能力，但可以用更少的参数描述更复杂的函数，并且在实践中效果更好。

第5章讨论了不同任务使用的损失函数，并结识了最小二乘损失的理论基础。第6章和第7章讨论了训练过程。第8章讨论了如何测量模型的性能。第9章讨论了用于提高模型性能的**正则化**（regularization）技术。

## Notes

**损失函数（loss function）与代价函数（cost function）**：在包括本书在内的大部分机器学习的书中，术语损失函数和代价函数是可以互换使用的。然而更准确地说，损失函数是与数据点相关的个别项，即公式2.5右侧求和公式中的一项；而代价函数是被最小化的整体，即公式2.5右侧的整个部分。一个代价函数可以包含与个别数据点无关的额外项，更多的细节参阅9.1节。更通俗的说，**目标函数**（objective function）是任何要被最大化或最小化的函数。

**生成模型（generative model）与判别模型（discriminative model）**：本章中使用的模型 $y = f[x, \phi]$ 是判别模型，这些模型利用从现实世界中测量到的数据 $x$ 进行预测 $y$。另一种方法是建立一个生成模型 $x = g[y, \phi]$，该模型的实际测量值 $x$ 被计算为输出 $y$​ 的函数。[Problems 2.3](#problems)

生成模型的缺点是它不能直接预测 $y$。为了进行推断，我们需要反转生成函数，即 $y = g^{-1}[x, \phi]$，然而这是一件艰难的事情。生成模型的优点在于我们可以内置关于数据生成方式的先验知识，例如要预测图像 $x$ 中汽车的三维位置和方向y，那我们可以将有关汽车形状，三维几何和光传播的知识构建到函数 $x = g[y, \phi]$​ 中。

生成模型看起来是绝妙的主意，但事实上，判别模型在现代机器学习中占主导地位。通过利用大量训练数据可以训练非常灵活的判别模型，往往能够胜过在生成模型中利用先验知识所获得的优势。

## Problems

* **problem 2.1**：为了在损失函数上“下坡”，我们需要计算相对于参数 $\phi_0$ 和 $\phi_1$ 的梯度。计算斜率 $\frac{\partial L}{\partial\phi_0}$ 和 $\frac{\partial L}{\partial\phi_1}$ 的表达式。

* **problem 2.2**：证明我们可以通过problem 2.1的导数表达式设置为0，并求解 $\phi_0$ 和 $\phi_1$ 来找到闭式损失函数的最小值。需要注意的是，这适用于线性回归，但不适用于更复杂的模型，这就是我们为什么使用梯度下降的方式拟合模型。
* **problem 2.3**：将线性回归模型重新表述为一个生成模型，因此可以得到 $x = g[y, \phi] = \phi_0 + \phi_1y$。新的损失函数是什么？找到一个用于进行推断的反函数表达式 $y = g^{-1}[x, \phi]$。这个模型对于给定的训练数据集 $\{x_i, y_i\}$ 是否回产生于判别模型相同的预测？一种验证方式是编写代码，用两种方式来拟合三个数据点的直线，并且查看结果是否相同。